{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02944396",
      "metadata": {
        "id": "02944396"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c9652bc6",
      "metadata": {
        "id": "c9652bc6"
      },
      "outputs": [],
      "source": [
        "# Import \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from degree_freedom_queen import *\n",
        "from degree_freedom_king1 import *\n",
        "from degree_freedom_king2 import *\n",
        "from generate_game import *\n",
        "from Chess_env import *\n",
        "from Policy import *\n",
        "\n",
        "size_board = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bceca7c",
      "metadata": {
        "id": "0bceca7c"
      },
      "source": [
        "#### The Environment\n",
        "\n",
        "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
        "Chess_env is composed by the following methods:\n",
        "\n",
        "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
        "\n",
        "     S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
        "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
        "     \n",
        "     X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
        "     \n",
        "     allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
        "     \n",
        "\n",
        "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
        "\n",
        "     R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
        "     \n",
        "     Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
        "     \n",
        "     \n",
        "3. Features. Given the chessboard position, the method computes the features.\n",
        "\n",
        "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9593a299",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9593a299"
      },
      "source": [
        "#### Initialize env & parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CxEn_WFNt0fG"
      },
      "outputs": [],
      "source": [
        "## INITIALISE THE ENVIRONMENT\n",
        "env = Chess_Env(size_board)\n",
        "# INITIALISE THE PARAMETERS OF YOUR NEURAL NETWORK AND...\n",
        "# PLEASE CONSIDER TO USE A MASK OF ONE FOR THE ACTION MADE AND ZERO OTHERWISE IF YOU ARE NOT USING VANILLA GRADIENT DESCENT...\n",
        "# WE SUGGEST A NETWORK WITH ONE HIDDEN LAYER WITH SIZE 200.\n",
        "from Q_values import Q_values\n",
        "from SARSA import sarsa\n",
        "from Double_Q import Double_Q\n",
        "\n",
        "np.random.seed(22)\n",
        "\n",
        "S, X, allowed_a = env.Initialise_game()\n",
        "\n",
        "N_in = np.shape(X)[0]  ## INPUT SIZE\n",
        "N_h = 200  ## NUMBER OF HIDDEN NODES\n",
        "N_a = np.shape(allowed_a)[0]  # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "\n",
        "# HYPERPARAMETERS SUGGESTED (FOR A GRID SIZE OF 4)\n",
        "epsilon_0 = 0.4 # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
        "beta = 0.00005     # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)\n",
        "gamma = 0.2  # THE DISCOUNT FACTOR\n",
        "eta = 0.02        # THE LEARNING RATE\n",
        "## for attaching label of output, e.g. data file.etc\n",
        "label_tag = \"eps_04_gamma_02\"\n",
        "\n",
        "## no need to change this part\n",
        "momentum = 0.7\n",
        "# how fast the weights copied from target network to online network\n",
        "double_q_update_coe = 0.1\n",
        "## INITALISE YOUR NEURAL NETWORK...\n",
        "q_value = Q_values([N_in, N_h, N_a])\n",
        "q_value.nn.momentum = momentum\n",
        "q_value.gamma = gamma\n",
        "\n",
        "sarsa_v = sarsa([N_in, N_h, N_a])\n",
        "sarsa_v.nn.momentum = momentum\n",
        "sarsa_v.gamma = gamma\n",
        "\n",
        "double_q_v = Double_Q([N_in, N_h, N_a])\n",
        "double_q_v.Q_primary.nn.momentum = momentum\n",
        "double_q_v.gamma = gamma\n",
        "double_q_v.update_coefficient = double_q_update_coe\n",
        "\n",
        "N_episodes = 70000  # THE NUMBER OF GAMES TO BE PLAYED\n",
        "\n",
        "## to stop training when get a model of avg_R of 1 in validation Test\n",
        "early_stop = False\n",
        "N_threshold = 2.1\n",
        "## turn on experience replay\n",
        "EXP_REPLAY = False"
      ],
      "id": "CxEn_WFNt0fG"
    },
    {
      "cell_type": "markdown",
      "id": "9d2bb7bd",
      "metadata": {
        "id": "9d2bb7bd"
      },
      "source": [
        "#### Common Functions (non-training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PnMKjm8zt0fJ"
      },
      "outputs": [],
      "source": [
        "## choose action with the largest value. Used in validation not in training update function.\n",
        "## if no positive action is allowed, ramdonly choose one.\n",
        "## during training process, this case can happend in validation.\n",
        "def agent_action(value_function_model, X, allowed_a):\n",
        "    (V, _) = value_function_model(X)\n",
        "    allow_v = np.copy(V)\n",
        "    allow_v[np.where(allowed_a.flatten() != 1)] = 0\n",
        "    if (np.max(allow_v) == 0):\n",
        "        return np.random.permutation(np.where(allowed_a.flatten() == 1)[0])[0]\n",
        "    else:\n",
        "        return np.argmax(allow_v)\n",
        "# play one game and return R and steps\n",
        "def play_one_game(value_function, max_step=16):\n",
        "    S, X, allowed_a = env.Initialise_game()  # INITIALISE GAME\n",
        "    Done = 0  # SET Done=0 AT THE BEGINNING\n",
        "    i = 1  # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
        "    while Done == 0:\n",
        "        a_agent = agent_action(value_function, X, allowed_a)\n",
        "        S, X, allowed_a, R, Done = env.OneStep(a_agent)\n",
        "        if Done:\n",
        "            return (R, i)\n",
        "        if i > max_step:\n",
        "            return (R, i)\n",
        "        i = i + 1\n",
        "\n",
        "## validate performance\n",
        "def period_validate(function_model, validate_games):\n",
        "    # SAVING VARIABLES\n",
        "    R_save_temp = np.zeros([validate_games, 1])\n",
        "    N_moves_save_temp = np.zeros([validate_games, 1])\n",
        "    for n in range(validate_games):\n",
        "        (R, n_moves) = play_one_game(function_model)\n",
        "        R_save_temp[n] = np.copy(R)\n",
        "        N_moves_save_temp[n] = np.copy(n_moves)\n",
        "    return (np.mean(R_save_temp), np.mean(N_moves_save_temp))\n",
        "\n",
        "## exp moving average\n",
        "def EMA(x, y):\n",
        "    k = 50 / len(x)\n",
        "    ema_seq = []\n",
        "    ema_prev = 0\n",
        "    for t in range(0, len(x)):\n",
        "        y_t = y[t] * k + ema_prev * (1 - k)\n",
        "        ema_prev = y_t\n",
        "        ema_seq.append(y_t)\n",
        "    return (x, ema_seq)\n",
        "\n",
        "import pickle\n",
        "def save_model(name=\"saved_model.pkl\"):\n",
        "    with open(f\"{name}\", 'wb') as output:\n",
        "        pickle.dump(sarsa_v, output, pickle.HIGHEST_PROTOCOL)\n",
        "        pickle.dump(q_value, output, pickle.HIGHEST_PROTOCOL)\n",
        "        pickle.dump(double_q_v, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_model(name=\"saved_model.pkl\"):\n",
        "    with open(f\"{name}\", 'rb') as input:\n",
        "        loaded_sarsa_v = pickle.load(input)\n",
        "        loaded_q_value = pickle.load(input)\n",
        "        loaded_double_q_v = pickle.load(input)\n",
        "        return (loaded_sarsa_v, loaded_q_value, loaded_double_q_v)"
      ],
      "id": "PnMKjm8zt0fJ"
    },
    {
      "cell_type": "markdown",
      "id": "eba703fc",
      "metadata": {
        "id": "eba703fc"
      },
      "source": [
        "#### Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "503572f4",
      "metadata": {
        "id": "503572f4"
      },
      "outputs": [],
      "source": [
        "## this function looks somehow not so elegent.  \n",
        "# history sequence [((x_t, allowed_next, done), action_t, r_t)]\n",
        "\n",
        "def experience_replay(history, agent, model_next_q, model_value_function, eta):\n",
        "    ## sample from history\n",
        "    his_copy = np.copy(history)\n",
        "    shuffle = np.random.permutation((len(his_copy)-1))\n",
        "    sample_num = np.random.randint(1, (len(history)-1))\n",
        "    \n",
        "    ## mini batch\n",
        "    update = []\n",
        "    for i in range(sample_num):\n",
        "        his_item = his_copy[shuffle[i]]\n",
        "        x = his_item[0][0]\n",
        "        r = his_item[2]\n",
        "        partial = []\n",
        "        if (i+1 >= len(history) or his_item[0][2]==1):\n",
        "            ## last step in history\n",
        "            (predict_q, neuron_val) = model_value_function(x)\n",
        "            temp_delta = r - predict_q\n",
        "            ## feed agent.nn get partial\n",
        "            partial = agent.nn.partial_derivative(agent.nn.W_bias, temp_delta, neuron_val)\n",
        "        else:\n",
        "\n",
        "            (predict_q, neuron_val)=model_value_function(x)\n",
        "\n",
        "            next = his_copy[shuffle[i]+1]\n",
        "           \n",
        "            next_x = next[0][0]\n",
        "            allowed_action = next[0][1]\n",
        "\n",
        "            ## value + neuron value\n",
        "            (next_value, _) = model_value_function(next_x)\n",
        "            ##  R + \n",
        "            y = (r +  model_next_q(next_value, allowed_action, next_x))\n",
        "            temp_delta = y - predict_q\n",
        "            partial = agent.nn.partial_derivative(\n",
        "                agent.nn.W_bias, temp_delta, neuron_val\n",
        "            )\n",
        "        if(len(update) == 0):\n",
        "            update = partial\n",
        "            continue\n",
        "        for pi in range(len(partial)):\n",
        "            update[pi]= (np.add(update[pi][0], partial[pi][0]), np.add(update[pi][1], partial[pi][1]))\n",
        "\n",
        "    for ui in range(len(update)):\n",
        "        update[ui] = (update[ui][0]/sample_num, update[ui][1]/sample_num)\n",
        "    agent.nn.momentum_gradient_decent_update(eta, update)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e91dc20",
      "metadata": {
        "id": "1e91dc20"
      },
      "source": [
        "#### Traning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_ZtMhHBMt0fL"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "his_size = 30\n",
        "\n",
        "import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def train(N_episodes, epsilon_0, agent, model_value_function, model_training_function, model_calculate_next_R):\n",
        "    # global sarsa_v\n",
        "    # TRAINING LOOP BONE STRUCTURE\n",
        "    R_period_seq = []\n",
        "    N_period_step_seq = []\n",
        "    x_period_axis = []\n",
        "\n",
        "    R_training_seq = []\n",
        "    N_training_seq = []\n",
        "    x_training_seq = []\n",
        "\n",
        "    hist = deque([], maxlen = his_size)\n",
        "\n",
        "    for n in range(N_episodes):\n",
        "        epsilon_f = epsilon_0 / (1 + beta * n)  ## DECAYING EPSILON\n",
        "        sarsa_v.epsilon = epsilon_f\n",
        "\n",
        "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
        "        \n",
        "        S, X, allowed_a = env.Initialise_game()\n",
        "\n",
        "        while Done == 0:  ## START THE EPISODE\n",
        "            ## value for all actions from current value function\n",
        "            (value_s, neuron_value) = model_value_function(X)\n",
        "            ## based on the alue, choose an action\n",
        "            a_agent = epsilon_greedy(value_s, np.asarray(allowed_a), epsilon_f)\n",
        "            ## feed action into env, get next state\n",
        "            S_next, X_next, allowed_a_next, R, Done = env.OneStep(a_agent)\n",
        "            \n",
        "            ## save history for experince replay\n",
        "            ## ((x_t, allowed_action), action_t, r_t)\n",
        "            hist.append(((X, allowed_a, Done), a_agent, R))\n",
        "            \n",
        "            if Done == 1:\n",
        "                ## No need to calculate future s' value\n",
        "                model_training_function(eta, neuron_value, a_agent, R, value_s)\n",
        "\n",
        "                ## experience replay\n",
        "                if (EXP_REPLAY and len(hist) == his_size):\n",
        "                    experience_replay(hist, agent, model_calculate_next_R, model_value_function, eta )\n",
        "                break\n",
        "            else:\n",
        "                ## evaluate value of next state, get value for all possibile actions\n",
        "                (V_next, _) = model_value_function(X_next)\n",
        "                ## calculate future value. \n",
        "                ## e.g. for SARSA use epsilon greedy to choose an action and get its value*gamma)\n",
        "                future_R = model_calculate_next_R(V_next, allowed_a_next, X_next)\n",
        "                \n",
        "                ## train the model, equal to SGD update\n",
        "                ## inside the function, model calculate delta and do gradient descent.\n",
        "                model_training_function(eta, neuron_value, a_agent, future_R, value_s)\n",
        "\n",
        "            # NEXT STATE AND CO. BECOME ACTUAL STATE...\n",
        "       \n",
        "            S = np.copy(S_next)\n",
        "            X = np.copy(X_next)\n",
        "            allowed_a = np.copy(allowed_a_next)\n",
        "\n",
        "        ############# record training data###############\n",
        "        (R, n_moves) = play_one_game(model_value_function)\n",
        "        # data of each episodes\n",
        "        x_training_seq.append(n)\n",
        "        R_training_seq.append(R)\n",
        "        N_training_seq.append(n_moves)\n",
        "        #######################################\n",
        "\n",
        "        ######### periodly validate performance ########################\n",
        "        if (n % 1000 == 0):\n",
        "            (avg_R, avg_n) = period_validate(model_value_function, 200)\n",
        "            print(f\"\\r Training process {np.round(n / N_episodes * 100, 2)}%  \"\n",
        "                  f\"avg_R: {np.round(avg_R, 2)}, avg_steps:{np.round(avg_n, 2)}\", end=\"\", flush=True)\n",
        "            x_period_axis.append(n)\n",
        "            R_period_seq.append(avg_R)\n",
        "            N_period_step_seq.append(avg_n)\n",
        "            # early stop to preserve best model so far\n",
        "            if (early_stop and avg_R > 0.99 and avg_n < N_threshold):\n",
        "                return [(x_period_axis, R_period_seq, N_period_step_seq),\n",
        "                        (x_training_seq, R_training_seq, N_training_seq)]\n",
        "        ############################################################\n",
        "\n",
        "    return [(x_period_axis, R_period_seq, N_period_step_seq),\n",
        "            (x_training_seq, R_training_seq, N_training_seq)]\n"
      ],
      "id": "_ZtMhHBMt0fL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YtUA3VRst0fM"
      },
      "source": [
        "#### Q Learning Training"
      ],
      "id": "YtUA3VRst0fM"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "4ghFe4GTt0fM",
        "outputId": "2881549c-5945-464c-cfd2-c9861cbc1438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r Training process 0.0%  avg_R: 0.22, avg_steps:7.09"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4f4cbce08a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0;32mlambda\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqv_s\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqv_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0;31m## model choose next action and future reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                         \u001b[0;32mlambda\u001b[0m \u001b[0mV_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_next\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_next\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_next_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                         )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mQ_x_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_R_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_N_step_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_training_his\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-903c971ad16e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(N_episodes, epsilon_0, agent, model_value_function, model_training_function, model_calculate_next_R)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m############# record training data###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_moves\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_value_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;31m# data of each episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mx_training_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-197dd68b487a>\u001b[0m in \u001b[0;36mplay_one_game\u001b[0;34m(value_function, max_step)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0ma_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-197dd68b487a>\u001b[0m in \u001b[0;36magent_action\u001b[0;34m(value_function_model, X, allowed_a)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## during training process, this case can happend in validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_function_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_function_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mallow_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mallow_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallowed_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4f4cbce08a08>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      2\u001b[0m Q_training_his = train(N_episodes, epsilon_0, q_value,\n\u001b[1;32m      3\u001b[0m                         \u001b[0;31m## model value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         \u001b[0;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                         \u001b[0;31m## model gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0;32mlambda\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqv_s\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqv_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Q_values.py\u001b[0m in \u001b[0;36mq_values\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# output, neuron_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_q_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/NeuralNet.py\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mW_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# vector.dot w => vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "Q_training_his = train(N_episodes, epsilon_0, q_value,\n",
        "                        ## model value function\n",
        "                        lambda X: q_value.q_values(X),\n",
        "                        ## model gradient descent \n",
        "                        lambda eta, neuron_value, a_agent, R, qv_s: q_value.update_q_func(eta, neuron_value, a_agent, R, qv_s),\n",
        "                        ## model choose next action and future reward\n",
        "                        lambda V_next, allowed_next,X_next: q_value.calculate_next_Q(V_next, allowed_next, X_next)\n",
        "                        )\n",
        "(Q_x_axis, Q_R_seq, Q_N_step_seq) = Q_training_his[0]\n",
        "(Q_x_training_seq, Q_R_training_seq, Q_N_training_seq) = Q_training_his[1]\n",
        "print()"
      ],
      "id": "4ghFe4GTt0fM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7QbJ2gNut0fN"
      },
      "source": [
        "#### SARSA Training"
      ],
      "id": "7QbJ2gNut0fN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RN0NURSdt0fN"
      },
      "outputs": [],
      "source": [
        "Sarsa_his = train(N_episodes, epsilon_0, sarsa_v,\n",
        "                  lambda X: sarsa_v.sarsa_value(X),\n",
        "                  lambda eta, neuron_value, a_agent, R, qv_s: sarsa_v.update_sarsa_func(eta, neuron_value, a_agent, R,\n",
        "                                                                                        qv_s),\n",
        "                  lambda V_next, allowed_next, X_next: sarsa_v.calculate_next_V(V_next, allowed_next, X_next)\n",
        "                  )\n",
        "(Sarsa_x_axis, Sarsa_R_seq, Sarsa_N_step_seq) = Sarsa_his[0]\n",
        "(Sarsa_x_training_seq, Sarsa_R_training_seq, Sarsa_N_training_seq) = Sarsa_his[1]\n",
        "print()"
      ],
      "id": "RN0NURSdt0fN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G5BbKpJ3t0fN"
      },
      "source": [
        "#### Double-Q Training"
      ],
      "id": "G5BbKpJ3t0fN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dgOdrNFpt0fO"
      },
      "outputs": [],
      "source": [
        "# double_q_his = train(N_episodes, epsilon_0, double_q_v,\n",
        "#                      lambda X: double_q_v.q_primary_value(X),\n",
        "#                      lambda eta, neuron_value, a_agent, R, qv_s: double_q_v.update_func(eta, neuron_value, a_agent, R,\n",
        "#                                                                                         qv_s),\n",
        "#                      lambda V_next, allowed_next, X_next: double_q_v.next_action_Q(V_next, allowed_next, X_next)\n",
        "#                      )\n",
        "# (dq_x_axis, dq_R_seq, dq_N_step_seq) = double_q_his[0]\n",
        "# (dq_x_training_seq, dq_R_training_seq, dq_N_training_seq) = double_q_his[1]\n",
        "# print()"
      ],
      "id": "dgOdrNFpt0fO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "W_Ahzmsyt0fO"
      },
      "source": [
        "#### Test Model"
      ],
      "id": "W_Ahzmsyt0fO"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "WssDNIKgt0fO",
        "outputId": "6bf5d6e6-5b26-4170-b703-6cfd8968a529"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2c98718cab43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_moves_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Q_agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msarsa_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarsa_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sarsa_agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# test(lambda X_in: double_q_v.q_primary_value(X_in), \"Double Q\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c98718cab43>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(agent, agent_tag)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m## no max step limit amost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mR_save\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mN_moves_save\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-197dd68b487a>\u001b[0m in \u001b[0;36mplay_one_game\u001b[0;34m(value_function, max_step)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0ma_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-197dd68b487a>\u001b[0m in \u001b[0;36magent_action\u001b[0;34m(value_function_model, X, allowed_a)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## during training process, this case can happend in validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_function_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_function_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mallow_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mallow_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallowed_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c98718cab43>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(X_in)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_moves_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Q_agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msarsa_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarsa_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sarsa_agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# test(lambda X_in: double_q_v.q_primary_value(X_in), \"Double Q\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Q_values.py\u001b[0m in \u001b[0;36mq_values\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# output, neuron_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_q_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/NeuralNet.py\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mW_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# vector.dot w => vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import sys, importlib\n",
        "importlib.reload(sys.modules['Chess_env'])\n",
        "\n",
        "def test(agent, agent_tag):\n",
        "    N_episodes = 20000\n",
        "    R_save = np.zeros([N_episodes, 1])\n",
        "    N_moves_save = np.zeros([N_episodes, 1])\n",
        "    for n in range(N_episodes):\n",
        "        ## no max step limit amost\n",
        "        (R, i) = play_one_game(agent, 100)\n",
        "        R_save[n] = np.copy(R)\n",
        "        N_moves_save[n] = np.copy(i)\n",
        "    print(f'Agent {agent_tag}, Average reward:', np.mean(R_save), 'Number of steps: ', np.mean(N_moves_save))\n",
        "    return (np.mean(R_save), np.mean(N_moves_save))\n",
        "\n",
        "test(lambda X_in: q_value.q_values(X_in), \"Q_agent\")\n",
        "test(lambda X_in: sarsa_v.sarsa_value(X_in), \"Sarsa_agent\")\n",
        "# test(lambda X_in: double_q_v.q_primary_value(X_in), \"Double Q\")\n"
      ],
      "id": "WssDNIKgt0fO"
    },
    {
      "cell_type": "markdown",
      "id": "3ea422c5",
      "metadata": {
        "id": "3ea422c5"
      },
      "source": [
        "#### Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3570bfec",
      "metadata": {
        "id": "3570bfec"
      },
      "outputs": [],
      "source": [
        "# to save model of the above 3, enable this line\n",
        "save_model(f\"model_{label_tag}.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0BzH8A-1t0fQ"
      },
      "source": [
        "#### Best Model performance"
      ],
      "id": "0BzH8A-1t0fQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ybgow_ait0fQ"
      },
      "outputs": [],
      "source": [
        "## load saved model\n",
        "# (loaded_sarsa_v, loaded_q_value, loaded_double_q_v) = load_model(\"model_9995.pkl\")\n",
        "# test(lambda X_in: loaded_q_value.q_values(X_in), \"Q_agent\")\n",
        "# test(lambda X_in: loaded_sarsa_v.sarsa_value(X_in), \"Sarsa_agent\")\n",
        "# test(lambda X_in: loaded_double_q_v.q_primary_value(X_in), \"Double Q\")"
      ],
      "id": "Ybgow_ait0fQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run saved mdel"
      ],
      "metadata": {
        "id": "VRHuZGwsiYpJ"
      },
      "id": "VRHuZGwsiYpJ"
    },
    {
      "cell_type": "code",
      "source": [
        "b_list=[\"00100\", \"00200\", \"00300\", \"00500\", \"00700\", \"01000\",\"02000\",\"04000\",\"10000\",\"30000\"]\n",
        "g_list=[\"001\",\"02\",\"07\",\"040\",\"055\", \"85\", \"99\"]\n",
        "model_names = []\n",
        "for b in b_list:\n",
        "    model_names.append(f\"gamma_07_eps_04_beta_{b}\")\n",
        "for g in g_list:\n",
        "    model_names.append(f\"eps_04_gamma_{g}\")\n",
        "  \n",
        "f = open(\"model_test.py\", \"a\")\n",
        "for m in model_names:\n",
        "    (loaded_sarsa_v, loaded_q_value, loaded_double_q_v) = load_model(f\"model_{m}.pkl\")\n",
        "    (q_r, q_n) = test(lambda X_in: loaded_q_value.q_values(X_in), f\"model_{m} Q\")\n",
        "    (s_r, s_n) = test(lambda X_in: loaded_sarsa_v.sarsa_value(X_in), f\"model_{m} Sarsa\")\n",
        "    qr_o = f\"{m}_q_test_r={str(q_r)}\"\n",
        "    sr_o =f\"{m}_sarsa_test_r={str(s_r)}\"\n",
        "    qn_o = f\"{m}_q_test_n={str(q_n)}\"\n",
        "    sn_o = f\"{m}_sarsa_test_n={str(s_n)}\"\n",
        "    o = [qr_o, sr_o, qn_o, sn_o]\n",
        "    for s in o:\n",
        "      f.write(s+\"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "acDFDb1XiXxK",
        "outputId": "791a2524-511f-4769-9b1e-b80ef0cc538a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "acDFDb1XiXxK",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent model_gamma_07_eps_04_beta_00100 Q, Average reward: 0.9972 Number of steps:  2.0701\n",
            "Agent model_gamma_07_eps_04_beta_00100 Sarsa, Average reward: 0.9995 Number of steps:  2.15485\n",
            "Agent model_gamma_07_eps_04_beta_00200 Q, Average reward: 0.99995 Number of steps:  2.11465\n",
            "Agent model_gamma_07_eps_04_beta_00200 Sarsa, Average reward: 0.99595 Number of steps:  2.36245\n",
            "Agent model_gamma_07_eps_04_beta_00300 Q, Average reward: 0.9999 Number of steps:  2.2497\n",
            "Agent model_gamma_07_eps_04_beta_00300 Sarsa, Average reward: 1.0 Number of steps:  2.21885\n",
            "Agent model_gamma_07_eps_04_beta_00500 Q, Average reward: 0.9999 Number of steps:  2.09455\n",
            "Agent model_gamma_07_eps_04_beta_00500 Sarsa, Average reward: 0.99645 Number of steps:  2.13425\n",
            "Agent model_gamma_07_eps_04_beta_00700 Q, Average reward: 0.9999 Number of steps:  2.39895\n",
            "Agent model_gamma_07_eps_04_beta_00700 Sarsa, Average reward: 0.99975 Number of steps:  2.2511\n",
            "Agent model_gamma_07_eps_04_beta_01000 Q, Average reward: 0.9998 Number of steps:  2.36945\n",
            "Agent model_gamma_07_eps_04_beta_01000 Sarsa, Average reward: 1.0 Number of steps:  2.27755\n",
            "Agent model_gamma_07_eps_04_beta_02000 Q, Average reward: 0.99975 Number of steps:  2.6608\n",
            "Agent model_gamma_07_eps_04_beta_02000 Sarsa, Average reward: 0.99695 Number of steps:  2.2737\n",
            "Agent model_gamma_07_eps_04_beta_04000 Q, Average reward: 0.99515 Number of steps:  2.50115\n",
            "Agent model_gamma_07_eps_04_beta_04000 Sarsa, Average reward: 0.99995 Number of steps:  2.30955\n",
            "Agent model_gamma_07_eps_04_beta_10000 Q, Average reward: 0.9962 Number of steps:  2.4616\n",
            "Agent model_gamma_07_eps_04_beta_10000 Sarsa, Average reward: 0.99985 Number of steps:  2.541\n",
            "Agent model_gamma_07_eps_04_beta_30000 Q, Average reward: 0.99915 Number of steps:  2.6424\n",
            "Agent model_gamma_07_eps_04_beta_30000 Sarsa, Average reward: 0.99905 Number of steps:  2.39365\n",
            "Agent model_eps_04_gamma_001 Q, Average reward: 0.79955 Number of steps:  2.6294\n",
            "Agent model_eps_04_gamma_001 Sarsa, Average reward: 0.78395 Number of steps:  2.82705\n",
            "Agent model_eps_04_gamma_02 Q, Average reward: 0.87885 Number of steps:  2.07185\n",
            "Agent model_eps_04_gamma_02 Sarsa, Average reward: 0.84735 Number of steps:  2.01695\n",
            "Agent model_eps_04_gamma_07 Q, Average reward: 1.0 Number of steps:  1.97625\n",
            "Agent model_eps_04_gamma_07 Sarsa, Average reward: 0.99975 Number of steps:  2.0091\n",
            "Agent model_eps_04_gamma_040 Q, Average reward: 0.95455 Number of steps:  1.9563\n",
            "Agent model_eps_04_gamma_040 Sarsa, Average reward: 0.94965 Number of steps:  2.09255\n",
            "Agent model_eps_04_gamma_055 Q, Average reward: 0.9999 Number of steps:  1.97475\n",
            "Agent model_eps_04_gamma_055 Sarsa, Average reward: 0.99635 Number of steps:  2.1368\n",
            "Agent model_eps_04_gamma_85 Q, Average reward: 1.0 Number of steps:  2.01695\n",
            "Agent model_eps_04_gamma_85 Sarsa, Average reward: 1.0 Number of steps:  2.0171\n",
            "Agent model_eps_04_gamma_99 Q, Average reward: 0.9994 Number of steps:  3.7342\n",
            "Agent model_eps_04_gamma_99 Sarsa, Average reward: 0.99695 Number of steps:  4.9889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "NmzjzT_Nt0fQ"
      },
      "source": [
        "#### save training process data"
      ],
      "id": "NmzjzT_Nt0fQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DeA3wSGKt0fQ"
      },
      "outputs": [],
      "source": [
        "###\n",
        "\n",
        "data_file = f\"data_{label_tag}.py\"\n",
        "\n",
        "## save training process data\n",
        "def assemble_data(label, x, avg_R, avg_N, x_training_seq, R_training_seq, N_training_seq):\n",
        "    f_x=f\"{label}_x_axis={str(x)}\"\n",
        "    f_avg_R = f\"{label}_avg_R={str(avg_R)}\"\n",
        "    f_avg_N = f\"{label}_avg_N={str(avg_N)}\"\n",
        "    f_x_training_seq = f\"{label}_x_training_seq={str(x_training_seq)}\"\n",
        "    f_R_training_seq = f\"{label}_R_training_seq={str(R_training_seq)}\"\n",
        "    f_N_training_seq = f\"{label}_N_training_seq={str(N_training_seq)}\"\n",
        "    return [f_x, f_avg_R, f_avg_N, f_x_training_seq, f_R_training_seq, f_N_training_seq]\n",
        "\n",
        "f = open(data_file, \"a\")\n",
        "output = []\n",
        "Q_data = assemble_data(f\"{label_tag}_Q_Learning\", Q_x_axis, Q_R_seq, Q_N_step_seq,\n",
        "                       Q_x_training_seq, Q_R_training_seq, Q_N_training_seq )\n",
        "SARSA_data = assemble_data(f\"{label_tag}_SARSA\", Sarsa_x_axis, Sarsa_R_seq, Sarsa_N_step_seq,\n",
        "                           Sarsa_x_training_seq, Sarsa_R_training_seq, Sarsa_N_training_seq)\n",
        "# Dq_data = assemble_data(f\"{label_tag}_Double_Q\", dq_x_axis, dq_R_seq, dq_N_step_seq,\n",
        "#                         dq_x_training_seq, dq_R_training_seq, dq_N_training_seq)\n",
        "output.extend(Q_data)\n",
        "output.extend(SARSA_data)\n",
        "# output.extend(Dq_data)\n",
        "for s in output:\n",
        "    f.write(s+\"\\n\")\n",
        "    pass\n",
        "f.close()"
      ],
      "id": "DeA3wSGKt0fQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LYPVz5bit0fR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot(title):\n",
        "    plt.figure(1)\n",
        "    plt.title(f\"{title} Validation R value\")\n",
        "    plt.plot(Q_Learning_x_axis, Q_Learning_avg_R, label=\"Q Learning\")\n",
        "    plt.plot(SARSA_x_axis, SARSA_avg_R, label=\"SARSA\")\n",
        "    # plt.plot(Double_Q_x_axis, Double_Q_avg_R, label=\"Double Q\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"episodes\")\n",
        "    plt.ylabel(\"R\")\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.title(f\"{title} Validation Checkmate steps\")\n",
        "    plt.plot(Q_Learning_x_axis, Q_Learning_avg_N, label=\"Q learning\")\n",
        "    plt.plot(SARSA_x_axis, SARSA_avg_N, label=\"SARSA\")\n",
        "    # plt.plot(Double_Q_x_axis, Double_Q_avg_N, label=\"Double Q\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"episodes\")\n",
        "    plt.ylabel(\"steps\")\n",
        "\n",
        "    k = (len(SARSA_x_training_seq))\n",
        "    (Sarsa_ema_x, Sarsa_R_ema) = EMA(SARSA_x_training_seq, SARSA_R_training_seq)\n",
        "    (Q_ema_x, Q_R_ema) = EMA(Q_Learning_x_training_seq, Q_Learning_R_training_seq)\n",
        "    # (Dq_ema_x, Dq_R_ema) = EMA(Double_Q_x_training_seq, Double_Q_R_training_seq)\n",
        "    (_, Sarsa_N_ema) = EMA(SARSA_x_training_seq, SARSA_N_training_seq)\n",
        "    (_, Q_N_ema) = EMA(Q_Learning_x_training_seq, Q_Learning_N_training_seq)\n",
        "    # (_, Dq_N_ema) = EMA(Double_Q_x_training_seq, Double_Q_N_training_seq)\n",
        "\n",
        "    plt.figure(3)\n",
        "    plt.title(f\"{title} R trend\")\n",
        "    plt.plot(Q_ema_x, Q_R_ema, label=\"Q learning\")\n",
        "    plt.plot(Sarsa_ema_x, Sarsa_R_ema, label=\"SARSA\")\n",
        "    # plt.plot(Dq_ema_x, Dq_R_ema, label=\"Double Q\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"episodes\")\n",
        "    plt.ylabel(\"R\")\n",
        "\n",
        "\n",
        "    plt.figure(4)\n",
        "    plt.title(f\"{title} steps trend\")\n",
        "    plt.plot(Q_ema_x, Q_N_ema, label=\"Q learning\")\n",
        "    plt.plot(Sarsa_ema_x, Sarsa_N_ema, label=\"SARSA\")\n",
        "    # plt.plot(Dq_ema_x, Dq_N_ema , label=\"Double Q\")\n",
        "    plt.xlabel(\"episodes\")\n",
        "    plt.ylabel(\"steps\")\n",
        "    plt.legend()\n",
        "\n"
      ],
      "id": "LYPVz5bit0fR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7xTCWEwQt0fR"
      },
      "outputs": [],
      "source": [
        "Q_Learning_x_axis = Q_x_axis\n",
        "Q_Learning_avg_R = Q_R_seq\n",
        "Q_Learning_avg_N =  Q_N_step_seq\n",
        "SARSA_avg_N = Sarsa_N_step_seq\n",
        "# Double_Q_avg_N = dq_N_step_seq\n",
        "SARSA_x_axis = Sarsa_x_axis\n",
        "SARSA_avg_R = Sarsa_R_seq\n",
        "# Double_Q_x_axis = dq_x_axis\n",
        "# Double_Q_avg_R = dq_R_seq\n",
        "SARSA_x_training_seq = Sarsa_x_training_seq\n",
        "SARSA_R_training_seq = Sarsa_R_training_seq\n",
        "SARSA_N_training_seq = Sarsa_N_training_seq\n",
        "Q_Learning_x_training_seq = Q_x_training_seq\n",
        "Q_Learning_R_training_seq = Q_R_training_seq \n",
        "Q_Learning_N_training_seq = Q_N_training_seq\n",
        "# Double_Q_x_training_seq = dq_x_training_seq\n",
        "# Double_Q_R_training_seq = dq_R_training_seq\n",
        "# Double_Q_N_training_seq = dq_N_training_seq\n",
        "plot(\"-\")"
      ],
      "id": "7xTCWEwQt0fR"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zwq7PZlR1cMT"
      },
      "id": "Zwq7PZlR1cMT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}